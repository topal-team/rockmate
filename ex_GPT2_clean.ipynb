{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818dbb8-53d3-4417-94b4-fec607741ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import ModuleList\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "import torch.nn as nn\n",
    "import pytorch_checkmate as pk\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb45e7c-1ab2-4735-bf00-533fbed48d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nx, nf):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(size_out)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
    "        super().__init__()\n",
    "        self.c_fc    = Conv1D(d_model, nx)\n",
    "        self.c_proj  = Conv1D(nx, d_model)\n",
    "        self.act     = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head  = n_head\n",
    "        self.d_model = d_model\n",
    "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
    "        self.scale   = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.c_proj  = Conv1D(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"return shape [`batch`, `head`, `sequence`, `features`]\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head) \n",
    "        x = x.view(new_shape)\n",
    "        return x.permute(0, 2, 1, 3) \n",
    "    \n",
    "    def _attn(self, q, k, v, attn_mask=None):\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores  = self.softmax(scores)\n",
    "        scores  = self.dropout(scores)\n",
    "        outputs = torch.matmul(scores, v)\n",
    "        return outputs\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        x         = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
    "        return x.view(new_shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n",
    "        q, k, v  = x.split(self.d_model, dim=2)\n",
    "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        # out      = self._attn(q, k, v)\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
    "        # if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        # if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores  = self.softmax(scores)\n",
    "        scores  = self.dropout(scores)\n",
    "        out = torch.matmul(scores, v)\n",
    "        \n",
    "        out      = self.merge_heads(out)\n",
    "        out      = self.c_proj(out)\n",
    "        return out\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn        = Attention(d_model=d_model, n_head=n_head, d_head=64, n_ctx=1024, bias=True, scale=False, dropout=dropout)\n",
    "        self.feedforward = FeedForward(dropout=dropout, d_model=d_model, nx=d_model*4)\n",
    "        self.ln_1        = LayerNorm(d_model)\n",
    "        self.ln_2        = LayerNorm(d_model)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = self.ln_1(x)\n",
    "        x2 = self.ln_2(x)\n",
    "        x = x + self.attn(x1)\n",
    "        x = x + self.feedforward(x2)\n",
    "        return x\n",
    "    \n",
    "def _get_clones(module, n):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(n)])\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257, dropout=0.1):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        block        = TransformerBlock(d_model=d_model, n_head=12, dropout=dropout)\n",
    "        self.h       = _get_clones(block, nlayers)\n",
    "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
    "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop    = nn.Dropout(dropout)\n",
    "        self.ln_f    = LayerNorm(d_model)\n",
    "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.out.weight = self.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, src, labels=None, pos_ids=None, return_inp=False, dropout=0.1):\n",
    "        if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
    "        inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\n",
    "        if return_inp: return inp \n",
    "        for i in range(self.nlayers): inp = self.h[0](inp)\n",
    "        inp     = self.ln_f(inp)\n",
    "        logits  = self.out(inp)\n",
    "        outputs = (logits,) + (inp,)\n",
    "        \n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "            return outputs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ba95c-f90b-42f9-9df5-c7128eebd080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import GPT2Tokenizer\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#context1  = torch.tensor([tokenizer.encode(\"The planet earth\")])\n",
    "#context2  = torch.tensor([tokenizer.encode(\"I'm upset with those tools\")])\n",
    "context1 = torch.tensor([[ 464, 5440, 4534]])\n",
    "context2 = torch.tensor([[  40, 1101, 9247,  351,  883, 4899]])\n",
    "print(context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af07f6f-abc0-4e20-8143-f92ffa5477b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GPT2(nlayers=4,dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73073c-f030-4177-95bc-65d3e13faccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "GPT2_ag = pk.make_all_graphs(model2,{\"src\":context1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2abca-5742-4971-9740-1e6c6ac22eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_graphs(res,name,o):\n",
    "    pk.Dtools.print_D_graph(res.D_graph,name=f\"{name}_D_graph\",open=o)\n",
    "    pk.Stools.print_S_graph(res.S_graph,name=f\"{name}_S_graph\",open=o)\n",
    "    pk.Ktools.print_K_graph(res.K_graph,name=f\"{name}_K_graph\",open=o)\n",
    "    pk.Stools.print_S_graph_list(res.S_graph_list,name=f\"{name}_S_cut_graph\",open=o)\n",
    "    pk.Ktools.print_K_graph_list(res.K_graph_list,name=f\"{name}_K_cut_graph\",open=o)\n",
    "\n",
    "print_all_graphs(GPT2_ag,\"GPT2\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e584b84-c4e7-407e-b3e9-b8ec4eb3b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_mod = nn.Transformer(nhead=16, num_encoder_layers=1)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((10, 32, 512))\n",
    "tf_inputs = {\"src\":src,\"tgt\":tgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a19b27-a350-4db6-afaf-305e3368983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ag = pk.make_all_graphs(tf_mod,tf_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a03b4-6979-420b-8b48-c4c9fd721081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â print_all_graphs(tf_ag,\"nn.tf\",False) -> the K_graph is to complicated for graphviz to handle it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
