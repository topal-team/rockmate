{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/ygusak/anaconda3/envs/py310_neuralop/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import rkgb\n",
    "from rkgb.core.partitioned import PartitionerSequence, PartitionerBottomToTop\n",
    "import rockmate\n",
    "from rockmate import Rockmate\n",
    "from rockmate.solvers import HILP, RK_rotor\n",
    "from rockmate.solvers.main import add_sched\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ygusak/rockmate-private/')\n",
    "from models import get_nn_Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/ygusak/anaconda3/envs/py310_neuralop/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "device='cpu'\n",
    "\n",
    "batchsize = 8\n",
    "\n",
    "# model = torch.nn.Transformer(num_encoder_layers=1,num_decoder_layers=1)\n",
    "# sample = [\n",
    "#     torch.rand((200, batchsize, 512),device=device),\n",
    "#     torch.rand((200, batchsize, 512),device=device),\n",
    "# ]\n",
    "\n",
    "model, sample = get_nn_Transformer(\n",
    "    num_encoder_layers=1,\n",
    "    num_decoder_layers=1,\n",
    "    batchsize=8,\n",
    "    device = device)\n",
    "model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioner to get a sequence of blocks\n",
    "# sub_partitioner = PartitionerBottomToTop()\n",
    "# sub_partitioner.config.max_estimate_for_main_graph=10**7\n",
    "# sub_partitioner.config.max_estimate_per_sub_graph=10**6\n",
    "\n",
    "# partitioner = PartitionerSequence(sub_partitioner=sub_partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/ygusak/anaconda3/envs/py310_neuralop/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Build graphs based on the partitioner\n",
    "rkgb_res = rkgb.rkgb.Result(\n",
    "                    model,\n",
    "                    model_args=sample,\n",
    "                    # model_kwargs=model_kwargs,\n",
    "                    # verbose=verbose,\n",
    "                    # wanted_graphs={\"FB\"},\n",
    "                    # partitioners=[partitioner],\n",
    "                    inspection_device=torch.device(\"cuda\"),\n",
    "                    # print_time_in_each_stage=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hcn in rkgb_res.hierarchical_cluster.partitionings[0].list_HCNs:\n",
    "#     print(hcn.sub_cluster)\n",
    "#     try:\n",
    "#         print(hcn.sub_cluster.list_schedules)\n",
    "#     except:\n",
    "#         print('None cluster!!!!!!!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PULP_CBC_CMD to solve ILP\n",
      "Stage Raw took 00:00:03\n",
      "Stage Forward took 00:00:00\n",
      "Stage Simplifications took 00:00:00\n",
      "Stage Partitioning took 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/ygusak/anaconda3/envs/py310_neuralop/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/beegfs/ygusak/anaconda3/envs/py310_neuralop/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage Backward took 00:00:00\n",
      "Stage Hierarchical took 00:00:00\n"
     ]
    }
   ],
   "source": [
    "solver = HILP(ilp_solver=\"PULP_CBC_CMD\")\n",
    "solver.config.offload = False\n",
    "solver.config.solve_only_top_level = False\n",
    "solver.config.nb_total_nodes_top_level = 0\n",
    "rk_solver = RK_rotor()\n",
    "list_solvers = [solver, rk_solver]\n",
    "partitioners = [rkgb.partitioned.PartitionerSequence(\n",
    "        sub_partitioner=rkgb.partitioned.Partitioner())]\n",
    "\n",
    "budget = 5*10**8\n",
    "rkmod = Rockmate(\n",
    "        model,\n",
    "        sample,\n",
    "        budget=budget,\n",
    "        list_solvers=list_solvers,\n",
    "        rkgb_res=None,\n",
    "        solve_sched=False,\n",
    "        # verbose=False,\n",
    "        # ilp_solver=\"PULP_CBC_CMD\",\n",
    "        # ilp_time_limit=1 * 60 // 360,\n",
    "        # ilp_time_limit_top=10 * 60,\n",
    "        # model_kwargs=None,\n",
    "        partitioners=partitioners,\n",
    "        # max_size_S_graph_for_no_partitioning=40,\n",
    "        # cpu_optim = torch.optim.Adam,\n",
    "        # gpu_optim = torch.optim.Adam,\n",
    "        # optim_kwargs = {},\n",
    "        # minor_param_size = 10*1024**2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BottomHCluster(__3_addmm)\n",
      "[]\n",
      "BottomHCluster(__9_clone)\n",
      "[]\n",
      "H_Cluster_2_Ano_id_2\n",
      "[]\n",
      "BottomHCluster(__41_clone_1)\n",
      "[]\n",
      "BottomHCluster(__44_addmm_1)\n",
      "[]\n",
      "BottomHCluster(__47_getitem_2)\n",
      "[]\n",
      "BottomHCluster(__48_add)\n",
      "[]\n",
      "BottomHCluster(__50_getitem_4)\n",
      "[]\n",
      "H_Cluster_3_Ano_id_3\n",
      "[]\n",
      "BottomHCluster(__66_getitem_11)\n",
      "[]\n",
      "BottomHCluster(__68_getitem_14)\n",
      "[]\n",
      "H_Cluster_4_Ano_id_4\n",
      "[]\n",
      "BottomHCluster(__177_getitem_32)\n",
      "[]\n",
      "H_Cluster_5_Ano_id_3\n",
      "None\n",
      "BottomHCluster(__193_getitem_39)\n",
      "[]\n",
      "BottomHCluster(__195_getitem_42)\n",
      "[]\n",
      "None\n",
      "None cluster!!!!!!!\n",
      "\n",
      "BottomHCluster(__195_getitem_42)\n",
      "[]\n",
      "BottomHCluster(__193_getitem_39)\n",
      "[]\n",
      "H_Cluster_5_Ano_id_3\n",
      "None\n",
      "BottomHCluster(__177_getitem_32)\n",
      "[]\n",
      "H_Cluster_4_Ano_id_4\n",
      "[]\n",
      "BottomHCluster(__68_getitem_14)\n",
      "[]\n",
      "BottomHCluster(__66_getitem_11)\n",
      "[]\n",
      "H_Cluster_3_Ano_id_3\n",
      "[]\n",
      "BottomHCluster(__50_getitem_4)\n",
      "[]\n",
      "BottomHCluster(__48_add)\n",
      "[]\n",
      "BottomHCluster(__47_getitem_2)\n",
      "[]\n",
      "BottomHCluster(__44_addmm_1)\n",
      "[]\n",
      "BottomHCluster(__41_clone_1)\n",
      "[]\n",
      "H_Cluster_2_Ano_id_2\n",
      "[]\n",
      "BottomHCluster(__9_clone)\n",
      "[]\n",
      "BottomHCluster(__3_addmm)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for hcn in rkmod.rkgb_res.hierarchical_cluster.partitionings[0].list_HCNs:\n",
    "    print(hcn.sub_cluster)\n",
    "    try:\n",
    "        print(hcn.sub_cluster.list_schedules)\n",
    "    except:\n",
    "        print('None cluster!!!!!!!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rkmod.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BottomHCluster(__3_addmm)\n",
      "[Op_sched takes 0.71 ms with 18.75048828125 MiB peak mem, Op_sched takes 1.12 ms with 18.75048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__9_clone)\n",
      "[Op_sched takes 0.40 ms with 0.0 MiB peak mem, Op_sched takes 0.68 ms with 0.0 MiB peak mem] \n",
      "\n",
      "H_Cluster_2_Ano_id_2\n",
      "[Op_sched takes 2.01 ms with 50.87890625 MiB peak mem, Op_sched takes 2.77 ms with 50.87890625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__41_clone_1)\n",
      "[Op_sched takes 0.20 ms with 0.0 MiB peak mem, Op_sched takes 0.29 ms with 0.0 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__44_addmm_1)\n",
      "[Op_sched takes 0.44 ms with 6.25048828125 MiB peak mem, Op_sched takes 0.63 ms with 6.25048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__47_getitem_2)\n",
      "[Op_sched takes 0.16 ms with 1.5625 MiB peak mem, Op_sched takes 0.22 ms with 1.5625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__48_add)\n",
      "[Op_sched takes 0.15 ms with 0.0 MiB peak mem, Op_sched takes 0.20 ms with 0.0 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__50_getitem_4)\n",
      "[Op_sched takes 0.22 ms with 0.025390625 MiB peak mem, Op_sched takes 0.32 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_3_Ano_id_3\n",
      "[Op_sched takes 3.12 ms with 62.50048828125 MiB peak mem, Op_sched takes 4.33 ms with 62.50048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__66_getitem_11)\n",
      "[Op_sched takes 0.20 ms with 0.025390625 MiB peak mem, Op_sched takes 0.28 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__68_getitem_14)\n",
      "[Op_sched takes 0.21 ms with 0.025390625 MiB peak mem, Op_sched takes 0.31 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_4_Ano_id_4\n",
      "[Op_sched takes 8.80 ms with 152.1611328125 MiB peak mem, Op_sched takes 12.65 ms with 152.1611328125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__177_getitem_32)\n",
      "[Op_sched takes 0.21 ms with 0.025390625 MiB peak mem, Op_sched takes 0.32 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_5_Ano_id_3\n",
      "None schedule!!!!!!!\n",
      "\n",
      "BottomHCluster(__193_getitem_39)\n",
      "[Op_sched takes 0.20 ms with 0.025390625 MiB peak mem, Op_sched takes 0.28 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__195_getitem_42)\n",
      "[Op_sched takes 0.19 ms with 0.025390625 MiB peak mem, Op_sched takes 0.27 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "None\n",
      "None cluster!!!!!!!\n",
      "\n",
      "BottomHCluster(__195_getitem_42)\n",
      "[Op_sched takes 0.19 ms with 0.025390625 MiB peak mem, Op_sched takes 0.27 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__193_getitem_39)\n",
      "[Op_sched takes 0.20 ms with 0.025390625 MiB peak mem, Op_sched takes 0.28 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_5_Ano_id_3\n",
      "None schedule!!!!!!!\n",
      "\n",
      "BottomHCluster(__177_getitem_32)\n",
      "[Op_sched takes 0.21 ms with 0.025390625 MiB peak mem, Op_sched takes 0.32 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_4_Ano_id_4\n",
      "[Op_sched takes 8.80 ms with 152.1611328125 MiB peak mem, Op_sched takes 12.65 ms with 152.1611328125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__68_getitem_14)\n",
      "[Op_sched takes 0.21 ms with 0.025390625 MiB peak mem, Op_sched takes 0.31 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__66_getitem_11)\n",
      "[Op_sched takes 0.20 ms with 0.025390625 MiB peak mem, Op_sched takes 0.28 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_3_Ano_id_3\n",
      "[Op_sched takes 3.12 ms with 62.50048828125 MiB peak mem, Op_sched takes 4.33 ms with 62.50048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__50_getitem_4)\n",
      "[Op_sched takes 0.22 ms with 0.025390625 MiB peak mem, Op_sched takes 0.32 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__48_add)\n",
      "[Op_sched takes 0.15 ms with 0.0 MiB peak mem, Op_sched takes 0.20 ms with 0.0 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__47_getitem_2)\n",
      "[Op_sched takes 0.16 ms with 1.5625 MiB peak mem, Op_sched takes 0.22 ms with 1.5625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__44_addmm_1)\n",
      "[Op_sched takes 0.44 ms with 6.25048828125 MiB peak mem, Op_sched takes 0.63 ms with 6.25048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__41_clone_1)\n",
      "[Op_sched takes 0.20 ms with 0.0 MiB peak mem, Op_sched takes 0.29 ms with 0.0 MiB peak mem] \n",
      "\n",
      "H_Cluster_2_Ano_id_2\n",
      "[Op_sched takes 2.01 ms with 50.87890625 MiB peak mem, Op_sched takes 2.77 ms with 50.87890625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__9_clone)\n",
      "[Op_sched takes 0.40 ms with 0.0 MiB peak mem, Op_sched takes 0.68 ms with 0.0 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__3_addmm)\n",
      "[Op_sched takes 0.71 ms with 18.75048828125 MiB peak mem, Op_sched takes 1.12 ms with 18.75048828125 MiB peak mem] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for hcn in rkmod.rkgb_res.hierarchical_cluster.partitionings[0].list_HCNs:\n",
    "    print(hcn.sub_cluster)\n",
    "    if hcn.sub_cluster is None:\n",
    "        print('None cluster!!!!!!!\\n')\n",
    "    elif hcn.sub_cluster.list_schedules is None:\n",
    "        print('None schedule!!!!!!!\\n')\n",
    "    else:\n",
    "        print(hcn.sub_cluster.list_schedules, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solving H_Cluster_2_Ano_id_2\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "CHAIN SOLUTION!!!!!!!!!!!!!\n",
      "CHAIN SOLUTION!!!!!!!!!!!!!\n",
      "CHAIN SOLUTION!!!!!!!!!!!!!\n",
      "solving H_Cluster_3_Ano_id_3\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "CHAIN SOLUTION!!!!!!!!!!!!!\n",
      "CHAIN SOLUTION!!!!!!!!!!!!!\n",
      "CHAIN SOLUTION!!!!!!!!!!!!!\n",
      "solving H_Cluster_4_Ano_id_4\n",
      "solving H_Cluster_1_Ano_id_1\n",
      "CHAIN SOLUTION!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "cluster = rkmod.rkgb_res.hierarchical_cluster\n",
    "param_mem = sum(pnode.mem for pnode in cluster.parameter_nodes)\n",
    "param_grad_mem = sum(pnode.mem for pnode in cluster.parameter_nodes if pnode.info.requires_grad)\n",
    "act_budget = budget - param_mem - (1+rkmod.optimize_metrics[\"optimizer_states_size\"]) * param_grad_mem\n",
    "\n",
    "rkmod.solve_sched(act_budget)\n",
    "rkmod.get_compiled_fct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BottomHCluster(__3_addmm)\n",
      "BottomHCluster(__9_clone)\n",
      "H_Cluster_2_Ano_id_2\n",
      "Using PULP_CBC_CMD to solve ILP\n",
      "solving H_Cluster_2_Ano_id_2\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "BottomHCluster(__41_clone_1)\n",
      "BottomHCluster(__44_addmm_1)\n",
      "BottomHCluster(__47_getitem_2)\n",
      "BottomHCluster(__48_add)\n",
      "BottomHCluster(__50_getitem_4)\n",
      "H_Cluster_3_Ano_id_3\n",
      "Using PULP_CBC_CMD to solve ILP\n",
      "solving H_Cluster_3_Ano_id_3\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "BottomHCluster(__66_getitem_11)\n",
      "BottomHCluster(__68_getitem_14)\n",
      "H_Cluster_4_Ano_id_4\n",
      "Using PULP_CBC_CMD to solve ILP\n",
      "solving H_Cluster_4_Ano_id_4\n",
      "BottomHCluster(__177_getitem_32)\n",
      "H_Cluster_5_Ano_id_3\n",
      "Using PULP_CBC_CMD to solve ILP\n",
      "solving H_Cluster_5_Ano_id_3\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "Nb comp: 403, T:13\n",
      "time limit 900\n",
      "BottomHCluster(__193_getitem_39)\n",
      "BottomHCluster(__195_getitem_42)\n",
      "None\n",
      "BottomHCluster(__195_getitem_42)\n",
      "BottomHCluster(__193_getitem_39)\n",
      "H_Cluster_5_Ano_id_3\n",
      "BottomHCluster(__177_getitem_32)\n",
      "H_Cluster_4_Ano_id_4\n",
      "BottomHCluster(__68_getitem_14)\n",
      "BottomHCluster(__66_getitem_11)\n",
      "H_Cluster_3_Ano_id_3\n",
      "BottomHCluster(__50_getitem_4)\n",
      "BottomHCluster(__48_add)\n",
      "BottomHCluster(__47_getitem_2)\n",
      "BottomHCluster(__44_addmm_1)\n",
      "BottomHCluster(__41_clone_1)\n",
      "H_Cluster_2_Ano_id_2\n",
      "BottomHCluster(__9_clone)\n",
      "BottomHCluster(__3_addmm)\n"
     ]
    }
   ],
   "source": [
    "# rkmod.preprocess()\n",
    "\n",
    "for hcn in rkmod.rkgb_res.hierarchical_cluster.partitionings[0].list_HCNs:\n",
    "    print(hcn.sub_cluster)\n",
    "\n",
    "    if hcn.sub_cluster is None: continue\n",
    "    if not hcn.is_fwd: continue\n",
    "    if not hcn.sub_cluster.name.startswith('H'): continue\n",
    "    \n",
    "    solver = HILP(ilp_solver=\"PULP_CBC_CMD\")\n",
    "    solver.config.optimize_metrics = {\"minor_param_size\": 10*1024**2}\n",
    "\n",
    "    list_sched = solver(hcn.sub_cluster)\n",
    "    if hcn.sub_cluster.list_schedules is None:\n",
    "        hcn.sub_cluster.list_schedules = []\n",
    "    for sched in list_sched:\n",
    "        add_sched(hcn.sub_cluster, sched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BottomHCluster(__3_addmm)\n",
      "[Op_sched takes 0.72 ms with 18.75048828125 MiB peak mem, Op_sched takes 1.13 ms with 18.75048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__9_clone)\n",
      "[Op_sched takes 0.30 ms with 0.0 MiB peak mem, Op_sched takes 0.43 ms with 0.0 MiB peak mem] \n",
      "\n",
      "H_Cluster_2_Ano_id_2\n",
      "[Op_sched takes 2.25 ms with 50.87890625 MiB peak mem, Op_sched takes 3.21 ms with 50.87890625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__41_clone_1)\n",
      "[Op_sched takes 0.19 ms with 0.0 MiB peak mem, Op_sched takes 0.28 ms with 0.0 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__44_addmm_1)\n",
      "[Op_sched takes 0.44 ms with 6.25048828125 MiB peak mem, Op_sched takes 0.63 ms with 6.25048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__47_getitem_2)\n",
      "[Op_sched takes 0.16 ms with 1.5625 MiB peak mem, Op_sched takes 0.23 ms with 1.5625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__48_add)\n",
      "[Op_sched takes 0.15 ms with 0.0 MiB peak mem, Op_sched takes 0.20 ms with 0.0 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__50_getitem_4)\n",
      "[Op_sched takes 0.22 ms with 0.025390625 MiB peak mem, Op_sched takes 0.32 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_3_Ano_id_3\n",
      "[Op_sched takes 3.07 ms with 62.50048828125 MiB peak mem, Op_sched takes 4.28 ms with 62.50048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__66_getitem_11)\n",
      "[Op_sched takes 0.19 ms with 0.025390625 MiB peak mem, Op_sched takes 0.28 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__68_getitem_14)\n",
      "[Op_sched takes 0.21 ms with 0.025390625 MiB peak mem, Op_sched takes 0.30 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_4_Ano_id_4\n",
      "[Op_sched takes 8.86 ms with 148.64599609375 MiB peak mem, Op_sched takes 12.78 ms with 148.64599609375 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__177_getitem_32)\n",
      "[Op_sched takes 0.22 ms with 0.025390625 MiB peak mem, Op_sched takes 0.32 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_5_Ano_id_3\n",
      "None schedule!!!!!!!\n",
      "\n",
      "BottomHCluster(__193_getitem_39)\n",
      "[Op_sched takes 0.19 ms with 0.025390625 MiB peak mem, Op_sched takes 0.28 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__195_getitem_42)\n",
      "[Op_sched takes 0.19 ms with 0.025390625 MiB peak mem, Op_sched takes 0.27 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "None\n",
      "None cluster!!!!!!!\n",
      "\n",
      "BottomHCluster(__195_getitem_42)\n",
      "[Op_sched takes 0.19 ms with 0.025390625 MiB peak mem, Op_sched takes 0.27 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__193_getitem_39)\n",
      "[Op_sched takes 0.19 ms with 0.025390625 MiB peak mem, Op_sched takes 0.28 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_5_Ano_id_3\n",
      "None schedule!!!!!!!\n",
      "\n",
      "BottomHCluster(__177_getitem_32)\n",
      "[Op_sched takes 0.22 ms with 0.025390625 MiB peak mem, Op_sched takes 0.32 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_4_Ano_id_4\n",
      "[Op_sched takes 8.86 ms with 148.64599609375 MiB peak mem, Op_sched takes 12.78 ms with 148.64599609375 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__68_getitem_14)\n",
      "[Op_sched takes 0.21 ms with 0.025390625 MiB peak mem, Op_sched takes 0.30 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__66_getitem_11)\n",
      "[Op_sched takes 0.19 ms with 0.025390625 MiB peak mem, Op_sched takes 0.28 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "H_Cluster_3_Ano_id_3\n",
      "[Op_sched takes 3.07 ms with 62.50048828125 MiB peak mem, Op_sched takes 4.28 ms with 62.50048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__50_getitem_4)\n",
      "[Op_sched takes 0.22 ms with 0.025390625 MiB peak mem, Op_sched takes 0.32 ms with 0.025390625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__48_add)\n",
      "[Op_sched takes 0.15 ms with 0.0 MiB peak mem, Op_sched takes 0.20 ms with 0.0 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__47_getitem_2)\n",
      "[Op_sched takes 0.16 ms with 1.5625 MiB peak mem, Op_sched takes 0.23 ms with 1.5625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__44_addmm_1)\n",
      "[Op_sched takes 0.44 ms with 6.25048828125 MiB peak mem, Op_sched takes 0.63 ms with 6.25048828125 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__41_clone_1)\n",
      "[Op_sched takes 0.19 ms with 0.0 MiB peak mem, Op_sched takes 0.28 ms with 0.0 MiB peak mem] \n",
      "\n",
      "H_Cluster_2_Ano_id_2\n",
      "[Op_sched takes 2.25 ms with 50.87890625 MiB peak mem, Op_sched takes 3.21 ms with 50.87890625 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__9_clone)\n",
      "[Op_sched takes 0.30 ms with 0.0 MiB peak mem, Op_sched takes 0.43 ms with 0.0 MiB peak mem] \n",
      "\n",
      "BottomHCluster(__3_addmm)\n",
      "[Op_sched takes 0.72 ms with 18.75048828125 MiB peak mem, Op_sched takes 1.13 ms with 18.75048828125 MiB peak mem] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for hcn in rkgb_res.hierarchical_cluster.partitionings[0].list_HCNs:\n",
    "    print(hcn.sub_cluster)\n",
    "    if hcn.sub_cluster is None:\n",
    "        print('None cluster!!!!!!!\\n')\n",
    "    elif hcn.sub_cluster.list_schedules is None:\n",
    "        print('None schedule!!!!!!!\\n')\n",
    "    else:\n",
    "        print(hcn.sub_cluster.list_schedules, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rkmod.solve_sched(list_solvers=[RK_rotor()], recursive=False)\n",
    "\n",
    "solver = RK_rotor()\n",
    "solver.solve(rkmod.rkgb_res.hierarchical_cluster, [5*10**7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, [])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rkmod.op_sched, rkmod.list_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H_Cluster_1_Ano_id_1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcluster = rkmod.rkgb_res.hierarchical_cluster\n",
    "hcluster.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'simulate_update'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rkmod\u001b[39m.\u001b[39;49mget_compiled_fct()\n",
      "File \u001b[0;32m~/rockmate-private/rockmate/src/rockmate/rockmate.py:221\u001b[0m, in \u001b[0;36mRockmate.get_compiled_fct\u001b[0;34m(self, new_compiler)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_compiled_fct\u001b[39m(\u001b[39mself\u001b[39m, new_compiler\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mop_sched\u001b[39m.\u001b[39;49msimulate_update(Simulator, refine_optimize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m new_compiler:\n\u001b[1;32m    223\u001b[0m         storage \u001b[39m=\u001b[39m RK_Storage()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'simulate_update'"
     ]
    }
   ],
   "source": [
    "rkmod.get_compiled_fct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('py310_neuralop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af2a7e96c80e2e2b45049fcda42218b09fa7531d6bb67daf77b62bc020bdf108"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
